import streamlit as st
from datetime import datetime  # Import datetime for the footer
from utils.helpers import (
    display_expand_collapse_controls,
    expander_section,
    reset_expansion_state,
    reset_expand_collapse_triggers,
    reset_progress,
    save_progress,
    load_progress, inject_custom_css
)

PROGRESS_FILE = "progress.json"

def render():
    inject_custom_css()
    current_page = "Hallucinations"
    st.title("Hallucinations in Language Models")
    display_expand_collapse_controls(current_page)

    # --- Load progress from file ---
    if "read_sections" not in st.session_state:
        progress_data = load_progress()
        st.session_state["read_sections"] = set(progress_data.get("read_sections", []))
    # --- Define sections for progress tracking ---
    halluc_sections = {
        "What Are Hallucinations?": (
            "Hallucinations are **confident but incorrect responses** generated by a language model.\n\n"
            "Even though the response may sound fluent and factual, the model may be **making things up** — especially when it lacks context or isn’t grounded in verified data.\n\n"
            "#### Types of Hallucinations\n"
            "- **Factual Hallucinations:** Incorrect facts (e.g., wrong dates, names, or events).\n"
            "- **Citation Hallucinations:** Invented sources, URLs, or references.\n"
            "- **Logical Hallucinations:** Contradictions or flawed reasoning."
        ),
        "Startup Example": (
            "**Prompt:** “When was Stripe founded?”\n\n"
            "**LLM Output:** “Stripe was founded in 2015 in Toronto.” (Incorrect)\n\n"
            "**Correct Answer:** Stripe was founded in 2010 in San Francisco.\n\n"
            "For startups, hallucinations can lead to misinforming users, misrepresenting data in pitch decks, or publishing inaccurate content."
        ),
        "Why It Happens": (
            "Language models sometimes produce information that sounds correct but isn't. Here's why:\n\n"
            "- **LLMs generate language based on patterns in training data, not real-time internet access.**\n"
            "- **They don’t “know” facts — they predict the next likely word.**\n"
            "- **When uncertain, they may fabricate names, dates, citations, or product details.**"
        ),
        "How to Minimize": (
            "LLMs are powerful tools, but they can generate **confident-sounding yet incorrect information**. Here’s how to reduce the risk of hallucinations:\n\n"
            "- **Be specific with prompts:** Avoid vague instructions.\n"
            "- **Use retrieval-based methods (like RAG):** Combine LLMs with live or static knowledge sources.\n"
            "- **Manually review before publishing externally:** Always treat LLM responses as **first drafts**.\n"
            "- **Encourage uncertainty when appropriate:** Ask the model to cite sources or include phrases like *“I’m not sure”* when unsure."
        ),
        "Spot the Hallucination (Quiz)": None  # Placeholder for the quiz
    }

    # --- Sub-topic selector ---
    col_left, col_right = st.columns([3, 1])
    with col_left:
        st.markdown("### Understand and Detect AI Hallucinations")
    with col_right:
        halluc_subtopic = st.selectbox(
            "Sub-topic",
            ["All"] + list(halluc_sections.keys()),
            key="hallucination_subtopic"
        )

    # --- Display sections with expanders ---
    for title, content in halluc_sections.items():
        if halluc_subtopic == "All" or halluc_subtopic == title:
            with expander_section(title):
                # Header with checkbox on the right
                top_col_left, top_col_right = st.columns([5, 1])
                with top_col_left:
                    st.markdown(f"#### {title}")
                with top_col_right:
                    checkbox_key = f"read_checkbox_{title}"

                    # Initialize checkbox state if not already set
                    if checkbox_key not in st.session_state:
                        st.session_state[checkbox_key] = title in st.session_state["hallucination_read_sections"]

                    # Render the checkbox
                    completed = st.checkbox("Mark as complete", key=checkbox_key, value=st.session_state[checkbox_key])

                    # Sync read_sections with checkbox state
                    if completed:
                        st.session_state["hallucination_read_sections"].add(title)
                    else:
                        st.session_state["hallucination_read_sections"].discard(title)

                if title == "Spot the Hallucination (Quiz)":
                    # Interactive Quiz
                    st.markdown("#### Quick Check: Can You Spot the Hallucination?")
                    
                    q1 = st.radio("1. Which of the following is most likely a hallucination?", [
                        "-- Select an answer --",
                        "Google was founded in 1998.",
                        "Python was invented by Guido van Rossum.",
                        "OpenAI was acquired by Netflix in 2021."
                    ])
                    if q1 != "-- Select an answer --":
                        if q1 == "OpenAI was acquired by Netflix in 2021.":
                            st.success("Correct! That never happened — it’s a confident hallucination.")
                        else:
                            st.error("Incorrect. Try again.")

                    q2 = st.radio("2. True or False: Language models always know the facts.", [
                        "-- Select an answer --",
                        "True",
                        "False"
                    ])
                    if q2 != "-- Select an answer --":
                        if q2 == "False":
                            st.success("Correct! LLMs generate text based on patterns, not factual knowledge.")
                        else:
                            st.error("Incorrect. LLMs don’t always know the facts.")

                    q3 = st.radio("3. Which strategy helps reduce hallucinations?", [
                        "-- Select an answer --",
                        "Use vague prompts",
                        "Combine LLMs with retrieval-based methods",
                        "Avoid reviewing outputs"
                    ])
                    if q3 != "-- Select an answer --":
                        if q3 == "Combine LLMs with retrieval-based methods":
                            st.success("Correct! Retrieval-based methods help ground LLMs in factual data.")
                        else:
                            st.error("Incorrect. Try again.")
                else:
                    st.markdown(content)

    # --- Progress tracking ---
    total_sections = len(halluc_sections)
    read_sections = len(st.session_state["hallucination_read_sections"])
    progress = int((read_sections / total_sections) * 100)

    st.markdown("### Your Reading Progress")
    st.progress(progress)
    st.caption(f"You’ve completed **{read_sections} of {total_sections}** sections ({progress}%)")
    
    if st.button("Reset Progress"):
        reset_progress(halluc_sections)

    # Save progress to file whenever it changes
    save_progress()
    reset_expand_collapse_triggers()

    # --- Footer ---
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; font-size: 14px; line-height: 1.6;'>
            <strong>LLM Guide for Startups</strong> — Practical insights for using language models responsibly and efficiently in startup settings.<br>
            Built with ❤️ by:<br>
            • <strong>Vaishnavi Kandikonda</strong> — <a href="mailto:vaishnavi.kandikonda@ucdconnect.com">vaishnavi.kandikonda@ucdconnect.com</a><br>
            • <strong>Shivani Singh</strong> — <a href="mailto:shivani.singh@ucdconnect.ie">shivani.singh@ucdconnect.ie</a><br>
            • <strong>Kushal Pratap Singh</strong> — <a href="mailto:kushal.singh@ucdconnect.ie">kushal.singh@ucdconnect.ie</a><br><br>
            © 2025 LLM Startup Guide • Last updated {last_updated} • Built with Streamlit • Guided by principles of transparency, fairness, and human-centered AI.
        </div>
        """.format(last_updated=datetime.now().strftime("%Y-%m-%d")),
        unsafe_allow_html=True
    )